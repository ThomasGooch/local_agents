name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: [3.9, '3.10', 3.11, 3.12]
        
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt', 'pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        
    - name: Install system dependencies (Linux)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
        
    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/local_agents tests --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 src/local_agents tests --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
        
    - name: Type check with mypy
      run: |
        mypy src/local_agents --ignore-missing-imports --check-untyped-defs
        
    - name: Security check with bandit
      run: |
        bandit -r src/local_agents -f json -o bandit-report.json || true
        bandit -r src/local_agents -ll
        
    - name: Run unit tests
      run: |
        pytest tests/unit -v --tb=short -m "not slow" --cov=src/local_agents --cov-report=xml
        
    - name: Run integration tests
      run: |
        pytest tests/integration -v --tb=short -m "not slow" --timeout=300
        
    - name: Run performance tests (Linux only)
      if: runner.os == 'Linux' && matrix.python-version == '3.11'
      run: |
        pytest tests/performance -v --tb=short -m "performance and not slow" --timeout=600
        
    - name: Upload coverage to Codecov
      if: runner.os == 'Linux' && matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
        
    - name: Upload test artifacts
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: test-artifacts-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          htmlcov/
          tests.log
          bandit-report.json
          .pytest_cache/
          
  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety
        pip install -e ".[dev]"
        
    - name: Run Bandit security scanner
      run: |
        bandit -r src/local_agents -f json -o bandit-security-report.json
        bandit -r src/local_agents -ll
        
    - name: Run Safety dependency scanner
      run: |
        safety check --json --output safety-report.json || true
        safety check
        
    - name: Upload security artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-security-report.json
          safety-report.json
          
  compatibility-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        ollama-version: [latest, "0.1.17"]
        
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
        
    - name: Install Ollama
      run: |
        curl -fsSL https://ollama.ai/install.sh | sh
        
    - name: Start Ollama service
      run: |
        ollama serve &
        sleep 10
        
    - name: Pull test models
      run: |
        ollama pull llama2:7b-chat-q4_0 || echo "Model pull failed, tests will use mocks"
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        
    - name: Run compatibility tests
      run: |
        pytest tests/integration -v -m "ollama" --tb=short --timeout=600 || echo "Ollama integration tests skipped"
        
  performance-regression:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for comparison
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pytest-benchmark
        
    - name: Run performance benchmarks
      run: |
        pytest tests/performance -v --tb=short --benchmark-json=benchmark.json
        
    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        
  test-coverage-report:
    runs-on: ubuntu-latest
    needs: test
    if: always()
    
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        
    - name: Generate comprehensive coverage report
      run: |
        pytest tests/ --cov=src/local_agents --cov-report=html --cov-report=term --cov-report=xml --cov-fail-under=80
        
    - name: Upload coverage HTML report
      uses: actions/upload-artifact@v3
      with:
        name: coverage-html-report
        path: htmlcov/
        
    - name: Comment coverage on PR
      if: github.event_name == 'pull_request'
      uses: MishaKav/pytest-coverage-comment@main
      with:
        pytest-xml-coverage-path: ./coverage.xml
        junitxml-path: ./junit.xml